{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Do a little scraping or API-calling of your own.  Pick a new website and see what you can get out of it.  Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.  \n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)  \n",
    "2) Iterate over multiple pages/queries  \n",
    "3) Save the data to your computer  \n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest.  Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mercurynews.com\n",
      "CNBC\n",
      "USA Today\n",
      "Chicagotribune.com\n",
      "Bbc.com\n",
      "CNN\n",
      "The Wall Street Journal\n",
      "Nydailynews.com\n",
      "The Washington Post\n",
      "The Washington Post\n",
      "The New York Times\n",
      "Latimes.com\n",
      "Chicagotribune.com\n",
      "Reuters\n",
      "The Washington Post\n",
      "Npr.org\n",
      "The Wall Street Journal\n",
      "The Washington Post\n",
      "The Wall Street Journal\n",
      "NBC News\n",
      "Daily Mail\n",
      "The Guardian (AU)\n",
      "Independent\n",
      "The Economist\n",
      "The Economist\n",
      "Independent\n",
      "Independent\n",
      "Bloomberg\n",
      "Mirror\n",
      "Standard.co.uk\n",
      "Daily Mail\n",
      "The Guardian (AU)\n",
      "The Guardian (AU)\n",
      "Football365.com\n",
      "Daily Mail\n",
      "Standard.co.uk\n",
      "The Telegraph\n",
      "The Guardian (AU)\n",
      "Gloucestershirelive.co.uk\n",
      "Daily Mail\n",
      "Lenta\n",
      "Vesti.ru\n",
      "Fontanka.ru\n",
      "Interfax.ru\n",
      "RBC\n",
      "Tass.ru\n",
      "RBC\n",
      "Lenta\n",
      "Ria.ru\n",
      "Rambler.ru\n",
      "RBC\n",
      "Vesti.ru\n",
      "Ria.ru\n",
      "Riafan.ru\n",
      "Ria.ru\n",
      "Www.bfm.ru\n",
      "Tvrain.ru\n",
      "Rambler.ru\n",
      "RBC\n",
      "RBC\n",
      "Vox.com\n",
      "The Globe And Mail\n",
      "Nationalpost.com\n",
      "Bbc.com\n",
      "Fox News\n",
      "CBC News\n",
      "Fox News\n",
      "Thechronicleherald.ca\n",
      "The New York Times\n",
      "CBC News\n",
      "Huffingtonpost.ca\n",
      "CBC News\n",
      "The Globe And Mail\n",
      "The Washington Post\n",
      "Reuters\n",
      "Ctvnews.ca\n",
      "Anandtech.com\n",
      "The Globe And Mail\n",
      "Al.com\n",
      "Spokesman.com\n",
      "Francetvinfo.fr\n",
      "Francetvinfo.fr\n",
      "Bienpublic.com\n",
      "Le Monde\n",
      "Ledauphine.com\n",
      "Lefigaro.fr\n",
      "Lefigaro.fr\n",
      "20minutes.fr\n",
      "Leparisien.fr\n",
      "Lepoint.fr\n",
      "Francetvinfo.fr\n",
      "20minutes.fr\n",
      "Lefigaro.fr\n",
      "Francetvinfo.fr\n",
      "Leparisien.fr\n",
      "Lefigaro.fr\n",
      "Leparisien.fr\n",
      "Francebleu.fr\n",
      "20minutes.fr\n",
      "L'equipe\n",
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class SpidyQuotesSpider(scrapy.Spider):\n",
    "    name = 'newsapi'\n",
    "    start_urls = [\n",
    "        'https://newsapi.org/v2/top-headlines?country=us&apiKey=efd926684bba4292a267b8d2c8b52084',\n",
    "        'https://newsapi.org/v2/top-headlines?country=gb&apiKey=efd926684bba4292a267b8d2c8b52084',\n",
    "        'https://newsapi.org/v2/top-headlines?country=ru&apiKey=efd926684bba4292a267b8d2c8b52084',\n",
    "        'https://newsapi.org/v2/top-headlines?country=ca&apiKey=efd926684bba4292a267b8d2c8b52084',\n",
    "        'https://newsapi.org/v2/top-headlines?country=fr&apiKey=efd926684bba4292a267b8d2c8b52084']\n",
    " \n",
    "    def parse(self, response):\n",
    "        data = json.loads(response.body)\n",
    "        #print(data)\n",
    "        x = data['articles']\n",
    "        #print(x)\n",
    "        for each in x:\n",
    "            print(each['source']['name'])\n",
    "        for each in x:\n",
    "            yield {\n",
    "                    'title': each['source']['name']\n",
    "                    }\n",
    "                    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'NewsLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(SpidyQuotesSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "                 title\n",
      "95          Ctvnews.ca\n",
      "96       Anandtech.com\n",
      "97  The Globe And Mail\n",
      "98              Al.com\n",
      "99       Spokesman.com\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data \n",
    "\n",
    "News = pd.read_json('NewsLinks.json')\n",
    "print(News.shape)\n",
    "print(News.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from scipy import stats, integrate\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['20minutes.fr', 'Al.com', 'Anandtech.com', 'Bbc.com',\n",
       "        'Bienpublic.com', 'Bloomberg', 'CBC News', 'CNBC', 'CNN',\n",
       "        'Chicagotribune.com', 'Ctvnews.ca', 'Daily Mail', 'Fontanka.ru',\n",
       "        'Football365.com', 'Fox News', 'Francebleu.fr', 'Francetvinfo.fr',\n",
       "        'Gloucestershirelive.co.uk', 'Huffingtonpost.ca', 'Independent',\n",
       "        'Interfax.ru', \"L'equipe\", 'Latimes.com', 'Le Monde',\n",
       "        'Ledauphine.com', 'Lefigaro.fr', 'Lenta', 'Leparisien.fr',\n",
       "        'Lepoint.fr', 'Mercurynews.com', 'Mirror', 'NBC News',\n",
       "        'Nationalpost.com', 'Npr.org', 'Nydailynews.com', 'RBC',\n",
       "        'Rambler.ru', 'Reuters', 'Ria.ru', 'Riafan.ru', 'Spokesman.com',\n",
       "        'Standard.co.uk', 'Tass.ru', 'The Economist', 'The Globe And Mail',\n",
       "        'The Guardian (AU)', 'The New York Times', 'The Telegraph',\n",
       "        'The Wall Street Journal', 'The Washington Post',\n",
       "        'Thechronicleherald.ca', 'Tvrain.ru', 'USA Today', 'Vesti.ru',\n",
       "        'Vox.com', 'Www.bfm.ru'], dtype=object),\n",
       " array([3, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 4, 1, 1, 2, 1, 4, 1, 1, 3, 1, 1,\n",
       "        1, 1, 1, 4, 2, 3, 1, 1, 1, 1, 1, 1, 1, 5, 2, 2, 3, 1, 1, 2, 1, 2,\n",
       "        3, 4, 2, 1, 3, 5, 1, 1, 1, 2, 1, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(News, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = {}\n",
    "for each in News.title:\n",
    "    if each in news_dict:\n",
    "        news_dict[each] += 1\n",
    "    else:\n",
    "        news_dict[each] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20minutes.fr': 3,\n",
       " 'Al.com': 1,\n",
       " 'Anandtech.com': 1,\n",
       " 'Bbc.com': 2,\n",
       " 'Bienpublic.com': 1,\n",
       " 'Bloomberg': 1,\n",
       " 'CBC News': 3,\n",
       " 'CNBC': 1,\n",
       " 'CNN': 1,\n",
       " 'Chicagotribune.com': 2,\n",
       " 'Ctvnews.ca': 1,\n",
       " 'Daily Mail': 4,\n",
       " 'Fontanka.ru': 1,\n",
       " 'Football365.com': 1,\n",
       " 'Fox News': 2,\n",
       " 'Francebleu.fr': 1,\n",
       " 'Francetvinfo.fr': 4,\n",
       " 'Gloucestershirelive.co.uk': 1,\n",
       " 'Huffingtonpost.ca': 1,\n",
       " 'Independent': 3,\n",
       " 'Interfax.ru': 1,\n",
       " \"L'equipe\": 1,\n",
       " 'Latimes.com': 1,\n",
       " 'Le Monde': 1,\n",
       " 'Ledauphine.com': 1,\n",
       " 'Lefigaro.fr': 4,\n",
       " 'Lenta': 2,\n",
       " 'Leparisien.fr': 3,\n",
       " 'Lepoint.fr': 1,\n",
       " 'Mercurynews.com': 1,\n",
       " 'Mirror': 1,\n",
       " 'NBC News': 1,\n",
       " 'Nationalpost.com': 1,\n",
       " 'Npr.org': 1,\n",
       " 'Nydailynews.com': 1,\n",
       " 'RBC': 5,\n",
       " 'Rambler.ru': 2,\n",
       " 'Reuters': 2,\n",
       " 'Ria.ru': 3,\n",
       " 'Riafan.ru': 1,\n",
       " 'Spokesman.com': 1,\n",
       " 'Standard.co.uk': 2,\n",
       " 'Tass.ru': 1,\n",
       " 'The Economist': 2,\n",
       " 'The Globe And Mail': 3,\n",
       " 'The Guardian (AU)': 4,\n",
       " 'The New York Times': 2,\n",
       " 'The Telegraph': 1,\n",
       " 'The Wall Street Journal': 3,\n",
       " 'The Washington Post': 5,\n",
       " 'Thechronicleherald.ca': 1,\n",
       " 'Tvrain.ru': 1,\n",
       " 'USA Today': 1,\n",
       " 'Vesti.ru': 2,\n",
       " 'Vox.com': 1,\n",
       " 'Www.bfm.ru': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wall Street Journal\n",
      "The Washington Post\n",
      "Daily Mail\n",
      "The Guardian (AU)\n",
      "Independent\n",
      "RBC\n",
      "Ria.ru\n",
      "The Globe And Mail\n",
      "CBC News\n",
      "Francetvinfo.fr\n",
      "Lefigaro.fr\n",
      "20minutes.fr\n",
      "Leparisien.fr\n"
     ]
    }
   ],
   "source": [
    "for each in news_dict:\n",
    "    if news_dict[each] > 2:\n",
    "        print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "For this challenge I decided to use News API, https://newsapi.org/docs, as an access point for news articles from multiple sources around the world. This challenge was difficult for several reasons. The first being that the lesson had only shown how to access an xml API page rather than a JSON source. Next, I wasn't able to use the 'next page' code because this json API doesn't have a 'next page' button. And lastly, for some reason my computer was sometimes finding incorrect information when using the APIs. So there was a lot of trial and error to reach the point of identifying the 13 most common news sources for the top stories in the US, UK, Russia, Canada, and France. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
