{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have two new regression methods at your fingertips, it's time to give them a spin. In fact, for this challenge, let's put them together! Pick a dataset of your choice with a binary outcome and the potential for at least 15 features. If you're drawing a blank, the crime rates in 2013 dataset has a lot of variables that could be made into a modelable binary outcome.\n",
    "\n",
    "Engineer your features, then create three models. Each model will be run on a training set and a test-set (or multiple test-sets, if you take a folds approach). The models should be:\n",
    "\n",
    "1. Vanilla logistic regression\n",
    "2. Ridge logistic regression\n",
    "3. Lasso logistic regression\n",
    "\n",
    "In your report, evaluate all three models and decide on your best. Be clear about the decisions you made that led to these models (feature selection, regularization parameter selection, model evaluation criteria) and why you think that particular model is the best of the three. Also reflect on the strengths and limitations of regression as a modeling approach. Were there things you couldn't do but you wish you could have done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Normalizing the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Access the data file from the FBI: UCR \n",
    "dataset = pd.read_excel(\"NYCCrime.xls\", header=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the dataset into a DataFrame\n",
    "data = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename the group columns\n",
    "data.columns = ['City', 'Population', 'Violent_crime', 'Murder_manslaughter', 'Rape1', 'Rape', 'Robbery',\n",
    "                'Aggravated_assault', 'Property_crime', 'Burglary', 'Theft', 'Vehicle_theft', 'Arson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the unnecessary columns\n",
    "data = data.drop(['Rape1', 'City'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the last three rows with null values\n",
    "data = data.drop(data.index[348:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the Arson null values to 0. \n",
    "data.Arson = np.nan_to_num(data.Arson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_group = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to remove outlier data\n",
    "def reject_outliers(data, m=2):\n",
    "    return data[abs(data - np.mean(data)) < m * np.std(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter the continuous variables through the outlier removal function and then drop the null values. \n",
    "for group in data_group.loc[:, 'Population':]:\n",
    "    data_group[group] = reject_outliers(data_group[group], m=2)\n",
    "data_group = data_group.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Recode Property_crime column to binary.\n",
    "data_group['Property_crime'] = np.where(data_group['Property_crime']>=112, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Independent variables\n",
    "X = data_group.drop('Property_crime', axis=1, inplace=False)\n",
    "# Dependent variable\n",
    "Y = data_group['Property_crime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare a logistic regression classifier.\n",
    "lr = LogisticRegression(C=1e20)\n",
    "# Fit the variables to the logistic model.\n",
    "fit = lr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coefficients: \n",
      " [[ -1.06272199e-04   3.07708244e-02   2.63299447e-03   4.63583878e-03\n",
      "    2.39181562e-02  -4.16164977e-04  -8.39111576e-03   1.58498936e-02\n",
      "    6.32665645e-03  -3.57276926e-05]]\n",
      "\n",
      "Intercept: \n",
      " [-0.04937526]\n",
      "\n",
      " Logistic Accuracy by Property Crime\n",
      "Property_crime   0    1\n",
      "row_0                  \n",
      "0               83    4\n",
      "1               89  167\n",
      "\n",
      " Percentage accuracy\n",
      "0.728862973761\n",
      "\n",
      "Each Cross Validated R2 score: \n",
      " [ 0.9         0.88405797  0.95588235  0.72058824  0.69117647]\n",
      "\n",
      "Overall Logistic Regression R2: 0.83 (+/- 0.21)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nCoefficients: \\n', fit.coef_)\n",
    "print('\\nIntercept: \\n', fit.intercept_)\n",
    "\n",
    "logistic_pred_y = lr.predict(X)\n",
    "print('\\n Logistic Accuracy by Property Crime')\n",
    "print(pd.crosstab(logistic_pred_y, Y))\n",
    "\n",
    "print('\\n Percentage accuracy')\n",
    "print(lr.score(X, Y))\n",
    "\n",
    "score = cross_val_score(fit, X, Y, cv=5)\n",
    "print('\\nEach Cross Validated R2 score: \\n', score)\n",
    "print(\"\\nOverall Logistic Regression R2: %0.2f (+/- %0.2f)\\n\" % (score.mean(), score.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Ridge and Lasso models, how do I know which lambda to use? For each model I'm going to iterate through different lambdas to see which produces the best outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the different values of alpha to be tested\n",
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20, 1e2, 1e3, 1e4, 1e8, 1e10, 1e15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for the R2 values of the Ridge model\n",
    "Ridge_list = []\n",
    "\n",
    "# Iterate through each lambda value\n",
    "for i in alpha_ridge:\n",
    "    # Declare a Ridge logistic regression classifier.\n",
    "    ridge = LogisticRegression(penalty='l2', C=i)\n",
    "    # Fit the variables to the logistic model.\n",
    "    ridgefit = ridge.fit(X, Y)\n",
    "    # Appending the R2 value from each iteration to an outside list\n",
    "    Ridge_list.append(ridgefit.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49854227405247814,\n",
       " 0.49854227405247814,\n",
       " 0.49854227405247814,\n",
       " 0.67930029154518945,\n",
       " 0.72011661807580174,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329,\n",
       " 0.7288629737609329]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ridge_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list above, I can see that the Ridge R2 stop improving at a lambda of 1e-2. Now I am going to print out the coefficients, intercepts, accuracy and crossvalidation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare a logistic regression classifier.\n",
    "ridge = LogisticRegression(penalty='l2', C=1e-2)\n",
    "# Fit the variables to the logistic model.\n",
    "ridgefit = ridge.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Coefficients: \n",
      " [[ -1.05910349e-04   2.99280384e-02   2.56471122e-03   4.51514966e-03\n",
      "    2.32888548e-02  -4.40677249e-04  -8.09462632e-03   1.58302213e-02\n",
      "    6.15991498e-03  -3.42880697e-05]]\n",
      "\n",
      "Ridge Intercept: \n",
      " [-0.04810701]\n",
      "\n",
      " Ridge Logistic Accuracy by Property Crime\n",
      "Property_crime   0    1\n",
      "row_0                  \n",
      "0               83    4\n",
      "1               89  167\n",
      "\n",
      " Percentage accuracy\n",
      "0.728862973761\n",
      "\n",
      "Each Cross Validated R2 score: \n",
      " [ 0.81428571  0.72463768  0.77941176  0.72058824  0.69117647]\n",
      "\n",
      "Overall Ridge Logistic Regression R2: 0.75 (+/- 0.09)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nRidge Coefficients: \\n', ridgefit.coef_)\n",
    "print('\\nRidge Intercept: \\n', ridgefit.intercept_)\n",
    "\n",
    "ridge_pred_y = ridgefit.predict(X)\n",
    "print('\\n Ridge Logistic Accuracy by Property Crime')\n",
    "print(pd.crosstab(ridge_pred_y, Y))\n",
    "\n",
    "print('\\n Percentage accuracy')\n",
    "print(ridgefit.score(X, Y))\n",
    "\n",
    "score = cross_val_score(ridgefit, X, Y, cv=5)\n",
    "print('\\nEach Cross Validated R2 score: \\n', score)\n",
    "print(\"\\nOverall Ridge Logistic Regression R2: %0.2f (+/- %0.2f)\\n\" % (score.mean(), score.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for the R2 values of Lasso model\n",
    "Lasso_list = []\n",
    "\n",
    "# Iterate through each lambda value\n",
    "for i in alpha_ridge: \n",
    "    # Declare a logistic regression classifier.\n",
    "    lasso = LogisticRegression(penalty='l1', C=i)\n",
    "    # Fit the variables to the logistic model.\n",
    "    lassofit = lasso.fit(X, Y)\n",
    "    # Appending the R2 value from each iteration to an outside list\n",
    "    Lasso_list.append(lassofit.score(X, Y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.50145772594752192,\n",
       " 0.50145772594752192,\n",
       " 0.50145772594752192,\n",
       " 0.49854227405247814,\n",
       " 0.63265306122448983,\n",
       " 0.67346938775510201,\n",
       " 0.99125364431486884,\n",
       " 0.98833819241982512,\n",
       " 0.99416909620991256,\n",
       " 0.99125364431486884,\n",
       " 0.98833819241982512,\n",
       " 0.99125364431486884,\n",
       " 0.99125364431486884,\n",
       " 0.99125364431486884,\n",
       " 0.99125364431486884,\n",
       " 0.99125364431486884]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lasso_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list above, I can see that the Lasso R2 stop improving around a lambda of 10. Now I am going to print out the coefficients, intercepts, accuracy and crossvalidation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare a logistic regression classifier.\n",
    "lasso = LogisticRegression(penalty='l1', C=10)\n",
    "# Fit the variables to the logistic model.\n",
    "lassofit = lasso.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso Coefficients: \n",
      " [[ -1.50888520e-04   2.24946254e-02   1.07153502e+00  -1.92452802e-01\n",
      "    3.85397369e-02   2.26731147e-02   1.96318673e-01   1.39286617e-01\n",
      "    8.37299493e-02  -2.78537072e-01]]\n",
      "\n",
      "Lasso Intercept: \n",
      " [-15.64517878]\n",
      "\n",
      " Lasso Logistic Accuracy by Property Crime\n",
      "Property_crime    0    1\n",
      "row_0                   \n",
      "0               171    2\n",
      "1                 1  169\n",
      "\n",
      " Percentage accuracy\n",
      "0.991253644315\n",
      "\n",
      "Each Cross Validated R2 score: \n",
      " [ 0.98571429  0.97101449  0.95588235  0.97058824  0.97058824]\n",
      "\n",
      "Overall Lasso Logistic Regression R2: 0.97 (+/- 0.02)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nLasso Coefficients: \\n', lassofit.coef_)\n",
    "print('\\nLasso Intercept: \\n', lassofit.intercept_)\n",
    "\n",
    "lasso_pred_y = lassofit.predict(X)\n",
    "print('\\n Lasso Logistic Accuracy by Property Crime')\n",
    "print(pd.crosstab(lasso_pred_y, Y))\n",
    "\n",
    "print('\\n Percentage accuracy')\n",
    "print(lassofit.score(X, Y))\n",
    "\n",
    "score = cross_val_score(lassofit, X, Y, cv=5)\n",
    "print('\\nEach Cross Validated R2 score: \\n', score)\n",
    "print(\"\\nOverall Lasso Logistic Regression R2: %0.2f (+/- %0.2f)\\n\" % (score.mean(), score.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model had a moderate R-squared result. However, the accuracy of the logistic model shows that it produced more false positive results and didn't have consistent R-squared during cross validation. The ridge regression model had the worst improvement in results. While the Ridge accuracy is identical to the logistic model its cross validation had the lowest returns. Finally, the Lasso model performed best with its highest cross validated R-squared scores. It had very good accuracy and did not appear to be biased to any particular errors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
