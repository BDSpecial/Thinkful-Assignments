{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "from matplotlib.mlab import PCA as mlabPCA\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This drill uses data from the Lending Club to predict the state of a loan given some information about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path for my data.\n",
    "y2015 = pd.read_csv(\n",
    "    'LoanStats3d.csv',\n",
    "    skipinitialspace=True,\n",
    "    header=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert Interest Rate to numeric.\n",
    "y2015['int_rate'] = pd.to_numeric(y2015['int_rate'].str.strip('%'), errors='coerce')\n",
    "\n",
    "# Drop other columns with many unique variables\n",
    "y2015.drop(['url', 'emp_title', 'zip_code', 'earliest_cr_line', 'revol_util',\n",
    "            'sub_grade', 'addr_state', 'desc'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove two summary rows at the end that don't actually contain data.\n",
    "y2015 = y2015[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the lesson's example of the cross validation score for the accuracy of the tree. Here we're about 98% accurate.\n",
    "However, there are a few potential problems. In the lesson they didn't do much in the way of feature selection or model refinement. As such there are a lot of features in there that we don't really need. Some of them are actually quite impressively useless. There's also some variance in the scores. The fact that one gave us only 93% accuracy while others gave higher than 98 is concerning. This variance could be corrected by increasing the number of estimators. That will make it take even longer to run, however, and it is already quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9853252 ,  0.98594225,  0.98627437,  0.98684429,  0.98572752,\n",
       "        0.9836377 ,  0.98610748,  0.9843494 ,  0.9842544 ,  0.98342318])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "X = y2015.drop('loan_status', 1)\n",
    "Y = y2015['loan_status']\n",
    "X = pd.get_dummies(X)\n",
    "X = X.dropna(axis=1)\n",
    "\n",
    "cross_val_score(rfc, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRILL: Third Attempt\n",
    "So here's your task. Get rid of as much data as possible without dropping below an average of 90% accuracy in a 10-fold cross validation.\n",
    "You'll want to do a few things in this process. First, dive into the data that we have and see which features are most important. This can be the raw features or the generated dummies. You may want to use PCA or correlation matrices.\n",
    "Can you do it without using anything related to payment amount or outstanding principal? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I begin by dropping all the columns with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2015=y2015.dropna(axis=1,how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am approaching this drill by transforming my data with a PCA. I started by dropping the loan_status column and then changing the remaining features into the get_dummies dataframe. I  normalized the data. Then I start to recreate the PCA lesson by changing my data into a covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance Matrix:\n",
      " [[  1.00000237e+00   1.00000237e+00   9.99996862e-01 ...,   0.00000000e+00\n",
      "    4.63903124e-04  -4.63903124e-04]\n",
      " [  1.00000237e+00   1.00000237e+00   9.99996862e-01 ...,   0.00000000e+00\n",
      "    4.63903124e-04  -4.63903124e-04]\n",
      " [  9.99996862e-01   9.99996862e-01   1.00000237e+00 ...,   0.00000000e+00\n",
      "    5.02365659e-04  -5.02365659e-04]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  4.63903124e-04   4.63903124e-04   5.02365659e-04 ...,   0.00000000e+00\n",
      "    1.00000237e+00  -1.00000237e+00]\n",
      " [ -4.63903124e-04  -4.63903124e-04  -5.02365659e-04 ...,   0.00000000e+00\n",
      "   -1.00000237e+00   1.00000237e+00]]\n"
     ]
    }
   ],
   "source": [
    "X = y2015.drop('loan_status', 1)\n",
    "Y = y2015['loan_status']\n",
    "X = pd.get_dummies(X)\n",
    "# Normalize the data so that all variables have a mean of 0 and standard deviation\n",
    "# of 1.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# The NumPy covariance function assumes that variables are represented by rows,\n",
    "# not columns, so we transpose X.\n",
    "Xt = X.T\n",
    "Cx = np.cov(Xt)\n",
    "print('Covariance Matrix:\\n', Cx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now calculating the eigenvalues and eigenvectors. I then print out the total variance in the dataset explained by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of total variance in the dataset explained by each component calculated by hand.\n",
      " [  9.19335446e-02 +0.00000000e+00j   5.28102054e-02 +0.00000000e+00j\n",
      "   3.88057452e-02 +0.00000000e+00j   3.17492823e-02 +0.00000000e+00j\n",
      "   3.08569313e-02 +0.00000000e+00j   2.75093150e-02 +0.00000000e+00j\n",
      "   2.22510990e-02 +0.00000000e+00j   2.11214262e-02 +0.00000000e+00j\n",
      "   1.96625984e-02 +0.00000000e+00j   1.79196570e-02 +0.00000000e+00j\n",
      "   1.70009728e-02 +0.00000000e+00j   1.60684092e-02 +0.00000000e+00j\n",
      "   1.50882798e-02 +0.00000000e+00j   1.46596976e-02 +0.00000000e+00j\n",
      "   1.41459669e-02 +0.00000000e+00j   1.37780013e-02 +0.00000000e+00j\n",
      "   1.30693950e-02 +0.00000000e+00j   1.23621896e-02 +0.00000000e+00j\n",
      "   1.16908141e-02 +0.00000000e+00j   1.14870702e-02 +0.00000000e+00j\n",
      "   1.11402688e-02 +0.00000000e+00j   2.63699502e-03 +0.00000000e+00j\n",
      "   2.89134305e-03 +0.00000000e+00j   3.18198555e-03 +0.00000000e+00j\n",
      "   2.24057280e-03 +0.00000000e+00j   3.72478616e-03 +0.00000000e+00j\n",
      "   4.20128060e-03 +0.00000000e+00j   2.00928418e-03 +0.00000000e+00j\n",
      "   1.81519479e-03 +0.00000000e+00j   1.76059922e-03 +0.00000000e+00j\n",
      "   1.78781218e-03 +0.00000000e+00j   1.67548945e-03 +0.00000000e+00j\n",
      "   3.88868746e-03 +0.00000000e+00j   4.97400734e-03 +0.00000000e+00j\n",
      "   5.10077243e-03 +0.00000000e+00j   1.07646905e-02 +0.00000000e+00j\n",
      "   1.08433302e-03 +0.00000000e+00j   5.53206946e-03 +0.00000000e+00j\n",
      "   9.74943001e-04 +0.00000000e+00j   8.76190561e-04 +0.00000000e+00j\n",
      "   7.46490141e-04 +0.00000000e+00j   1.05242676e-02 +0.00000000e+00j\n",
      "   5.92906054e-03 +0.00000000e+00j   5.97351086e-03 +0.00000000e+00j\n",
      "   5.80844455e-04 +0.00000000e+00j   5.48292281e-04 +0.00000000e+00j\n",
      "   1.01464948e-02 +0.00000000e+00j   3.15471404e-04 +0.00000000e+00j\n",
      "   2.36167515e-04 +0.00000000e+00j   2.30421913e-04 +0.00000000e+00j\n",
      "   6.81199682e-03 +0.00000000e+00j   6.98582437e-03 +0.00000000e+00j\n",
      "   7.19279314e-03 +0.00000000e+00j   1.25942266e-04 +0.00000000e+00j\n",
      "   1.19624495e-04 +0.00000000e+00j   1.06245421e-04 +0.00000000e+00j\n",
      "   1.00323753e-04 +0.00000000e+00j   7.43906803e-03 +0.00000000e+00j\n",
      "   7.72813544e-03 +0.00000000e+00j   9.69335487e-03 +0.00000000e+00j\n",
      "   9.63491545e-03 +0.00000000e+00j   7.87940769e-03 +0.00000000e+00j\n",
      "   9.53661675e-03 +0.00000000e+00j   9.49484567e-03 +0.00000000e+00j\n",
      "   8.02958611e-03 +0.00000000e+00j   9.37194069e-03 +0.00000000e+00j\n",
      "   8.14515894e-03 +0.00000000e+00j   9.31679025e-03 +0.00000000e+00j\n",
      "   9.30042563e-03 +0.00000000e+00j   8.19116205e-03 +0.00000000e+00j\n",
      "   9.21467849e-03 +0.00000000e+00j   8.25185690e-03 +0.00000000e+00j\n",
      "   9.24093664e-03 +0.00000000e+00j   9.18066887e-03 +0.00000000e+00j\n",
      "   9.15699521e-03 +0.00000000e+00j   8.37772352e-03 +0.00000000e+00j\n",
      "   8.38807081e-03 +0.00000000e+00j   9.10765787e-03 +0.00000000e+00j\n",
      "   9.11603567e-03 +0.00000000e+00j   9.05792479e-03 +0.00000000e+00j\n",
      "   9.02868859e-03 +0.00000000e+00j   8.91626071e-03 +0.00000000e+00j\n",
      "   8.60841391e-03 +0.00000000e+00j   9.00836047e-03 +0.00000000e+00j\n",
      "   8.86336177e-03 +0.00000000e+00j   8.83329028e-03 +0.00000000e+00j\n",
      "   8.58058919e-03 +0.00000000e+00j   8.68579788e-03 +0.00000000e+00j\n",
      "   8.80501565e-03 +0.00000000e+00j   8.98401437e-03 +0.00000000e+00j\n",
      "   8.55006842e-03 +0.00000000e+00j   8.96267255e-03 +0.00000000e+00j\n",
      "   8.81007281e-03 +0.00000000e+00j   8.70230762e-03 +0.00000000e+00j\n",
      "   8.50190550e-03 +0.00000000e+00j   8.52136680e-03 +0.00000000e+00j\n",
      "   8.47772169e-03 +0.00000000e+00j   8.46111618e-03 +0.00000000e+00j\n",
      "   8.46294462e-03 +0.00000000e+00j   8.46696244e-03 +0.00000000e+00j\n",
      "   9.31557542e-06 +0.00000000e+00j   1.04890967e-07 +0.00000000e+00j\n",
      "   1.11791200e-08 +0.00000000e+00j   2.42899303e-09 +0.00000000e+00j\n",
      "   1.07307898e-16 +0.00000000e+00j   8.98142608e-17 +0.00000000e+00j\n",
      "   5.00795005e-17 +0.00000000e+00j  -3.29438394e-17 +0.00000000e+00j\n",
      "   1.68445884e-17 +0.00000000e+00j  -2.65334867e-17 +0.00000000e+00j\n",
      "  -6.18354774e-18 +0.00000000e+00j   6.02303181e-18 +0.00000000e+00j\n",
      "   3.50259339e-18 +0.00000000e+00j  -1.79808480e-18 +0.00000000e+00j\n",
      "   2.67381969e-18 +0.00000000e+00j   1.28924227e-18 +5.98844364e-19j\n",
      "   1.28924227e-18 -5.98844364e-19j   1.06569474e-33 +0.00000000e+00j\n",
      "   0.00000000e+00 +0.00000000e+00j   0.00000000e+00 +0.00000000e+00j]\n"
     ]
    }
   ],
   "source": [
    "# Calculating eigenvalues and eigenvectors.\n",
    "eig_val_cov, eig_vec_cov = np.linalg.eig(Cx)\n",
    "\n",
    "print(\n",
    "    'The percentage of total variance in the dataset explained by each',\n",
    "    'component calculated by hand.\\n',\n",
    "    eig_val_cov / sum(eig_val_cov)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a basic plot of the eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAl1EJEUo0EVEUsT/B3PSOpW9jYcEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12920a160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(eig_val_cov)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the eigenvector encodes the proportion of total variance explained by a component. The total variance is equal to the number of variables in the PCA. Thus, an Eigenvalue of 1 means that the component explains the same amount of variance as one variable. An eigenvalue greater than 1 is desirable, since a component with an eigenvalue of 1 adds no value beyond the information contained in any individual variable, and an eigenvalue of less than 1 is actually less efficient at conveying information than a variable by itself. Here I print out all the Eigenvalues that are greater than 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10.8481840263+0j)\n",
      "(6.23161903474+0j)\n",
      "(4.57908881191+0j)\n",
      "(3.74642420991+0j)\n",
      "(3.64112654392+0j)\n",
      "(3.24610687539+0j)\n",
      "(2.62563591759+0j)\n",
      "(2.49233420566+0j)\n",
      "(2.32019212581+0j)\n",
      "(2.11452454778+0j)\n",
      "(2.00611954919+0j)\n",
      "(1.89607678663+0j)\n",
      "(1.78042124046+0j)\n",
      "(1.72984842673+0j)\n",
      "(1.66922805543+0j)\n",
      "(1.62580801118+0j)\n",
      "(1.54219227269+0j)\n",
      "(1.45874183367+0j)\n",
      "(1.37951934157+0j)\n",
      "(1.3554774995+0j)\n",
      "(1.31455484047+0j)\n",
      "(1.27023649369+0j)\n",
      "(1.24186652867+0j)\n",
      "(1.19728923054+0j)\n",
      "(1.14381859037+0j)\n",
      "(1.13692272346+0j)\n",
      "(1.12532344898+0j)\n",
      "(1.12039444938+0j)\n",
      "(1.10589162716+0j)\n",
      "(1.0993838598+0j)\n",
      "(1.097452831+0j)\n",
      "(1.08733464384+0j)\n",
      "(1.09043311307+0j)\n",
      "(1.08332149878+0j)\n",
      "(1.08052800136+0j)\n",
      "(1.07470618134+0j)\n",
      "(1.07569476306+0j)\n",
      "(1.06883766335+0j)\n",
      "(1.06538778312+0j)\n",
      "(1.05212126177+0j)\n",
      "(1.01579525319+0j)\n",
      "(1.06298906036+0j)\n",
      "(1.04587917262+0j)\n",
      "(1.04233072856+0j)\n",
      "(1.01251192835+0j)\n",
      "(1.02492658407+0j)\n",
      "(1.0389943142+0j)\n",
      "(1.06011621276+0j)\n",
      "(1.00891046901+0j)\n",
      "(1.05759787213+0j)\n",
      "(1.03959106063+0j)\n",
      "(1.02687473784+0j)\n",
      "(1.00322723164+0j)\n",
      "(1.0055236707+0j)\n",
      "(1.00037353488+0j)\n"
     ]
    }
   ],
   "source": [
    "for each in eig_val_cov:\n",
    "    if each > 1:\n",
    "        print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the PCA lesson it says, \"We then take $P$, choose the eigenvectors corresponding to the most variance explained ($P_{var}$), and use it to transform the original data $X$ (via $P_{var}X$) into $Y$, a new simplified dataset with as many columns as eigenvectors in ($P_{var}$).\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create P, which we will use to transform Cx into Cy to get Y, the\n",
    "# dimensionally-reduced representation of X.\n",
    "P = eig_vec_cov[:, 0:40]\n",
    "\n",
    "# Transform X into the new features.\n",
    "X_new_features = P.T.dot(Xt)\n",
    "# Transpose these new features into the correct matrix\n",
    "new_features = X_new_features.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I take this new simplified dataset and use in the random forest classifier with my outcome of interest, loan status. After running this model several times, I learned that 40 eigenvectors were required to reach an accuracy of 90%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.900672  ,  0.90658245,  0.90612904,  0.91315808,  0.91343893,\n",
       "        0.91006673,  0.9037023 ,  0.89521932,  0.87462892,  0.8145439 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "cross_val_score(rfc, new_features, Y, cv=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
